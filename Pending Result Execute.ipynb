{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import timedelta\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define today and month\n",
    "today = '2024-05-29'\n",
    "month = '2024-04'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the pending result file to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pending dfs in a folder\n",
    "# dfs = load_all_pending_dfs(Path(\"data\",today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_pending_df(path_:str) -> pd.DataFrame:\n",
    "    \"\"\"Load pending result DataFrame\"\"\"\n",
    "    \n",
    "    df=pd.read_excel(path_,skiprows=1, engine='xlrd')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coba_load = load_single_pending_df(Path(\"data\",today,\"Pending Results.xls\"))\n",
    "# coba_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_pending_dfs(path_:str) -> list[pd.DataFrame]:\n",
    "    \"\"\"Load all pending result dataframe, return them as a list of dataframe\"\"\"\n",
    "    # Load dataframes for each file\n",
    "    offline_classroom = load_single_pending_df(Path(path_, \"Pending Results.xls\"))\n",
    "    offline_other = load_single_pending_df(Path(path_, \"Pending Results(1).xls\"))\n",
    "    online_classroom = load_single_pending_df(Path(path_, \"Pending Results(2).xls\"))\n",
    "    online_other = load_single_pending_df(Path(path_, \"Pending Results(3).xls\"))\n",
    "    \n",
    "    # Store dataframes in a list\n",
    "    return [offline_classroom, offline_other, online_classroom, online_other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = load_all_pending_dfs(Path(\"data\",today))\n",
    "# dfs[0].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data\n",
    "# df_clean = clean_pending_df(dfs,today,month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion clean_trainer_name\n",
    "# Note: in Series, we have to specified what we are work on. that why we must use str\n",
    "\n",
    "def clean_trainer_name(df:pd.DataFrame, teacher_col:str) -> pd.Series:\n",
    "    \"\"\"clean teacher's name\"\"\"\n",
    "    \n",
    "    teachers = (\n",
    "        df[teacher_col]\n",
    "        .str.title() #capital in the beginning of each word\n",
    "        .str.replace(\"\\(.+\\)\", \"\", regex=True) #replace data inside () with blank. ex: (Math)\n",
    "        .str.replace(\"\\s+\", \" \", regex=True) #Replace extra whitespace in the middle of words with single space\n",
    "        .str.strip() #Remove extra whitespace in the beginning and end of words\n",
    "        .replace(\n",
    "            {\n",
    "                \"Azhar Rahul\": \"Azhar Rahul Finaya\",\n",
    "                \"Handayani Risma\": \"Handayani Khaerunisyah Risma\",\n",
    "                \"Kartikasari Prettya\": \"Kartikasari Prettya Nur\",\n",
    "                \"Ramadhan Ira Ragil\": \"Ramadhani Ira\",\n",
    "                \"S Allan\": \"Santiago Allan\",\n",
    "                \"Gandhama Jesita\": \"Ghandama Jesita\",\n",
    "                \"Istiqomah Diah\": \"Toluhula Diah Istiqomah\",\n",
    "                \"Putri Tiara\": \"Setiawan Tiara Putri\",\n",
    "                \"Hamsah Ratnasari Handayani\": \"Hamsah Handayani Ratnasari\"\n",
    "                \n",
    "            }\n",
    "        )\n",
    "        \n",
    "    )\n",
    "    \n",
    "    return teachers\n",
    "\n",
    "#Call function pakai data frame coba_load untuk lihat perubahan data\n",
    "# coba_coba= clean_trainer_name(coba_load, \"Teacher\")\n",
    "# coba_coba.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function load_trainer_df\n",
    "def load_trainer_df(month: str) -> pd.DataFrame:\n",
    "    \"\"\"Load file trainer_working_days\"\"\"\n",
    "    \n",
    "    path= os.getenv(\"path_trainer_data\") #Path untuk lokasi file trainer_working_days ini ada di file .env\n",
    "    df_trainer= pd.read_excel(path, sheet_name=month) #nama sheet di file trainer_working_days itu sama dengan month yg di define di awal\n",
    "    \n",
    "    \n",
    "    return df_trainer\n",
    "\n",
    "#Call out the function\n",
    "# load_trainer_df(month).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data using chaining method\n",
    "def clean_pending_df(dfs: list, date_exported: str, month: str) -> pd.DataFrame:\n",
    "    \"\"\"Clean pending result dataframe to obtain list of pending results per session\"\"\"\n",
    "   \n",
    "    df_clean =(\n",
    "        pd.concat(dfs) # gabung 4 file pending result jadi satu\n",
    "        .drop( #hapus kolom\n",
    "            columns=[\"Unnamed: 6\",\"Level / Unit\", \"First Name\", \"Last Name\", \"Code\", \"Service Type\"]\n",
    "        )\n",
    "        .drop_duplicates( #hapus duplicated row untuk tahu jadwal kelas dari setiap teacher\n",
    "            subset=[\"Teacher\", \"Class Type\", \"Date\", \"Start Time\"], keep=\"first\"\n",
    "        )\n",
    "        .loc[lambda df_: df_[\"Class Type\"] != \"Staff Appointment\"]\n",
    "        .rename(columns = lambda c: c.lower().replace(\"_\",\"\"))\n",
    "        .dropna(subset=[\"teacher\"])\n",
    "        .dropna(how=\"all\",axis=0) #Delete empty rows\n",
    "        .dropna(how=\"all\", axis=1) #Delete empty column\n",
    "        .assign( #modify/overwritten the column name \"teacher\"\n",
    "            teacher=lambda df_: clean_trainer_name(\n",
    "                df_, \"teacher\" \n",
    "                # lambda function that takes df_ as input and applies the function clean_trainer_name() to the \"teacher\" column. \n",
    "            ),\n",
    "            date= lambda df_: pd.to_datetime(df_[\"date\"]), # Modify/overwritten the column 'date' to datetime format\n",
    "            date_exported= pd.to_datetime(date_exported) #Create New Column \"date_exported\"\n",
    "        )\n",
    "        # merge with et data to get area and position\n",
    "        .merge(\n",
    "            right=load_trainer_df(month), #Merge dfs (left table) with the result of load_trainer_df function (right table)\n",
    "            left_on= \"teacher\", # key join dari column di dfs\n",
    "            right_on=\"coco_teacher_name\", #Key join dari hasil function load_trainer_df\n",
    "            how=\"left\" #pakai Left join untuk merge data\n",
    "        )\n",
    "        #Drop unused columns\n",
    "        .drop(\n",
    "            columns=[\n",
    "                \"coco_teacher_name\",\n",
    "                \"erwin_teacher_name\",\n",
    "                \"teacher_working_days\",\n",
    "                \"teacher_note_1\",\n",
    "                \"teacher_note_2\",\n",
    "                \"center name\"\n",
    "            ]\n",
    "        )\n",
    "        # Sort columns\n",
    "        # tanda : artinya kita akan akses semua baris dari column yang disebutkan\n",
    "        .loc[\n",
    "            :,\n",
    "            [\n",
    "                \"teacher\",\n",
    "                \"date\",\n",
    "                \"start time\",\n",
    "                \"class type\",\n",
    "                \"teacher_position\",\n",
    "                \"teacher_center\",\n",
    "                \"teacher_area\",\n",
    "                \"date_exported\",\n",
    "            ],\n",
    "        ]\n",
    "        # sorts the DataFrame based on multiple columns in ascending order. \n",
    "        .sort_values([\"teacher_area\", \"teacher_center\", \"teacher\", \"date\"])\n",
    "        .reset_index(drop=True) # resets the index of the DataFrame after sorting and drops the old index.\n",
    "        # Exclude some ET name that already resigned\n",
    "        .loc[lambda df_: ~df_[\"teacher\"].isin([\"Jane Quinn Madeline\",\n",
    "            \"Priscilla Yokhebed\",\n",
    "            \"Rifani Aurora Nurhidayah\",\n",
    "            \"Nasarah Nadya\",\n",
    "             \"Louei Frentzen Caesar\",\n",
    "            \"Bushey James Michael\",\n",
    "            \"Mowatt Peter Denis\",\n",
    "             \"Azhar Rahul Finaya\",\n",
    "            \"Kartikasari Prettya Nur\",\n",
    "             \"Laurendeau Derek\",\n",
    "             \"Rozak Abdul Rahman\"])]\n",
    "        #Keep only the data from the last 365 days\n",
    "        .loc[lambda df_: df_[\"date\"] >= (pd.to_datetime(date_exported) - timedelta(days=365))]\n",
    "\n",
    "    )\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# df_clean = clean_pending_df(dfs,today,month)\n",
    "# df_clean.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cek how may duplicated rows\n",
    "# print(df_clean.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = clean_pending_df(dfs,today,month)\n",
    "# df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transform Data (Get summary of pending result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count pending result per month / create summary\n",
    "# df_pending = count_pending_result(df_clean, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function count_pending_result\n",
    "\n",
    "def count_pending_result(df: pd.DataFrame, today: str) -> pd.DataFrame:\n",
    "    \"\"\"Get summary of pending result in the last 365 days,\n",
    "    grouped by area and teacher, pivoted per month.\n",
    "    \"\"\"\n",
    "    \n",
    "    return(\n",
    "        df\n",
    "         # This filters the DataFrame to include only the data from the past 365 days.\n",
    "        .loc[lambda df_: (pd.to_datetime(today) - df_[\"date\"]).dt.days <= 365]\n",
    "        #Group the data and \"date\" column grouped by monthly frequency (1M)\n",
    "        .groupby([\"teacher_area\", \"teacher\", pd.Grouper(key=\"date\", freq=\"1M\")])\n",
    "        # counting how many times each unique teacher's name shows up in the \"teacher\" column within each group defined by \"teacher_area\" and the month\n",
    "        # save the output in new column called num_session_with_pending_res\n",
    "        .agg(num_session_with_pending_res=(\"teacher\",\"count\"))\n",
    "        #Karena teacher area & teacher column jadi index, kita reset\n",
    "        .reset_index()\n",
    "        #Pivot the table where teacher area & teacher jadi index, date jadi column dan num_session_with_pending_res jadi value\n",
    "        .pivot(index=[\"teacher_area\", \"teacher\"], columns=\"date\")\n",
    "        # # Replace all NaN elements with 0s.\n",
    "        .fillna(0)\n",
    "        # note: do not display trainer if the last 3 months pending results is 0\n",
    "        # .loc[lambda df_: df_.iloc[:, -3:].sum(axis=1) != 0]\n",
    "        # dropping the first (top) level of the column index, which contains the label \"num_session_with_pending_res\"\n",
    "        # axis =1 means we are working on the column level. drop the top level from the columns of the DataFrame.\n",
    "        .droplevel(0, axis=1)\n",
    "        #Sort the column based on the newest date to the oldest date\n",
    "        .sort_index(axis=\"columns\", ascending=False)\n",
    "        # renames each column label (which represents dates) into a more readable format\n",
    "        .rename(columns= lambda c: c.strftime(\"%b %Y\"))\n",
    "        \n",
    "        \n",
    "        # #Rename index level (teacher_area and teacher are become index here). rename_axis itu secara default 0 axisnya (change index level)\n",
    "        .rename_axis([\"Teacher Area\", \"Teacher\"])\n",
    "        #Rename the column \"date\" with blank. jadi cuma hapus tulisan \"date\" aja\n",
    "        .rename_axis([\"\"], axis=1)\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count pending result per month / create summary\n",
    "# df_pending = count_pending_result(df_clean, today)\n",
    "# df_pending.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load file file in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** file size (180835) not 512 + multiple of sector size (512)\n",
      "WARNING *** file size (1106531) not 512 + multiple of sector size (512)\n",
      "WARNING *** file size (6819) not 512 + multiple of sector size (512)\n",
      "WARNING *** file size (213603) not 512 + multiple of sector size (512)\n"
     ]
    }
   ],
   "source": [
    "# Load all pending dfs in a folder\n",
    "dfs= load_all_pending_dfs(Path(\"data\",today))\n",
    "\n",
    "#Clean data\n",
    "df_clean= clean_pending_df(dfs,today,month)\n",
    "\n",
    "#Count pending result per month/create summary\n",
    "df_pending= count_pending_result(df_clean, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check that all the teachers exist in coco trainer data\n",
    "\n",
    "def test_all_teacher_exist_in_coco_trainer_data(df_clean):\n",
    "    # Check if the teacher area is null, then show the teacher name\n",
    "    unmapped= df_clean.loc[df_clean[\"teacher_area\"].isna(), \"teacher\"].unique()\n",
    "    # Check if the result is zero or not. if not zero, it will print the list of teachers that is not list down in coco trainer data\n",
    "    assert(\n",
    "        unmapped.shape[0] == 0   \n",
    "    ), f\"some teacher are not listed in coco_trainer_data: {unmapped}\"\n",
    "    \n",
    "test_all_teacher_exist_in_coco_trainer_data(df_clean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['teacher', 'date', 'start time', 'class type', 'teacher_position',\n",
      "       'teacher_center', 'teacher_area', 'date_exported'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data per area\n",
    "df_clean= df_clean.rename(columns= lambda c: c.replace(\"_\",\" \").title()) #  replace underscores with spaces and title case the column names (capitalize the first letter of each word).\n",
    "\n",
    "# These lines create separate DataFrames for each teacher area.\n",
    "df_jkt1= df_clean.loc[df_clean[\"Teacher Area\"] == \"JKT 1\"]\n",
    "df_jkt2= df_clean.loc[df_clean[\"Teacher Area\"] == \"JKT 2\"]\n",
    "df_jkt3= df_clean.loc[df_clean[\"Teacher Area\"] == \"JKT 3\"]\n",
    "df_sby= df_clean.loc[df_clean[\"Teacher Area\"] == \"SBY\"]\n",
    "df_bdg= df_clean.loc[df_clean[\"Teacher Area\"] == \"BDG\"]\n",
    "df_onl= df_clean.loc[df_clean[\"Teacher Area\"].isin([\"Online\", \"Shared Account\", \"Ooolab\"])]\n",
    "df_oth= df_clean.loc[df_clean[\"Teacher Area\"].isin([\"Other\",\"HO\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check that the number of row that breakdown by the area is the same as the total row in df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This line asserts that the total number of rows in the original DataFrame df_clean is equal to the sum of rows in all the separate DataFrames created for each area.\n",
    "# It ensures that no rows are missed during the filtering process. If the total rows don't match, it will raise an error.\n",
    "# assert that no rows are missed\n",
    "assert len(df_clean) == sum(\n",
    "    [len(df) for df in [df_jkt1, df_jkt2, df_jkt3, df_sby, df_bdg, df_onl, df_oth]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397 397\n"
     ]
    }
   ],
   "source": [
    "# Check the total row of df_clean and the other df\n",
    "clean_row= len(df_clean)\n",
    "output_row= len(df_jkt1)+len(df_jkt2)+len(df_jkt3)+len(df_bdg)+len(df_sby)+len(df_onl)+len(df_oth)\n",
    "\n",
    "print(clean_row, output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teacher</th>\n",
       "      <th>Date</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>Class Type</th>\n",
       "      <th>Teacher Position</th>\n",
       "      <th>Teacher Center</th>\n",
       "      <th>Teacher Area</th>\n",
       "      <th>Date Exported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Teacher, Date, Start Time, Class Type, Teacher Position, Teacher Center, Teacher Area, Date Exported]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the data which is left out from the df_clean (if any)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_clean, df_jkt1, df_jkt2, df_jkt3, df_bdg, df_sby, df_onl, df_oth are your dataframes\n",
    "\n",
    "# Concatenate all other dataframes into one\n",
    "df_all_others = pd.concat([df_jkt1, df_jkt2, df_jkt3, df_bdg, df_sby, df_onl, df_oth])\n",
    "\n",
    "# Find rows in df_clean that are not in any other dataframe\n",
    "filtered_df = df_clean[~df_clean['Teacher'].isin(df_all_others['Teacher'])]\n",
    "\n",
    "# Now filtered_df contains the rows from df_clean that are not in any other dataframe\n",
    "\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df\n",
    "# write each dataframe to different worksheet\n",
    "\n",
    "filename= \"output.xlsx\"\n",
    "filepath= os.path.join(\"data\", today, filename)\n",
    "writer= pd.ExcelWriter(filepath, engine=\"xlsxwriter\")\n",
    "\n",
    "# These lines write each DataFrame to a separate worksheet within the Excel file.\n",
    "df_pending.to_excel(writer,sheet_name=\"Summary\", index=True)\n",
    "df_clean.to_excel(writer, sheet_name=\"All Area\", index=False)\n",
    "df_jkt1.to_excel(writer, sheet_name=\"JKT 1\", index=False)\n",
    "df_jkt2.to_excel(writer, sheet_name=\"JKT 2\", index=False)\n",
    "df_jkt3.to_excel(writer, sheet_name=\"JKT 3\", index=False)\n",
    "df_sby.to_excel(writer, sheet_name=\"SBY\", index=False)\n",
    "df_bdg.to_excel(writer, sheet_name=\"BDG\", index=False)\n",
    "df_onl.to_excel(writer, sheet_name=\"Online\", index=False)\n",
    "df_oth.to_excel(writer, sheet_name=\"Other\", index=False)\n",
    "\n",
    "writer.close()\n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
